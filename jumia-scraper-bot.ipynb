{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007087,
     "end_time": "2024-03-30T21:42:48.602050",
     "exception": false,
     "start_time": "2024-03-30T21:42:48.594963",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<b><font size='6'>Import Packages</font></b><br>\n",
    "<font size='4'>Setup Notebook Environment.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T05:27:38.114972Z",
     "iopub.status.busy": "2024-04-19T05:27:38.114547Z",
     "iopub.status.idle": "2024-04-19T05:27:38.122008Z",
     "shell.execute_reply": "2024-04-19T05:27:38.120614Z",
     "shell.execute_reply.started": "2024-04-19T05:27:38.114944Z"
    },
    "papermill": {
     "duration": 4.469813,
     "end_time": "2024-03-30T21:42:53.079047",
     "exception": false,
     "start_time": "2024-03-30T21:42:48.609234",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# WebScraping\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Google Cloud BigQuery\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Data Manipulation & Exploration\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# Time Intervals\n",
    "import time     \n",
    "\n",
    "# Set DateTime Values\n",
    "from datetime import datetime\n",
    "\n",
    "# WebScraping Progress Bar\n",
    "from tqdm import trange\n",
    "\n",
    "# Import OS\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006479,
     "end_time": "2024-03-30T21:42:53.092213",
     "exception": false,
     "start_time": "2024-03-30T21:42:53.085734",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<b><font size='6'>Initialize Client Object</font></b><br>\n",
    "<font size='4'>Provide path to google cloud credentials and initialize client object.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'adrianjuliusaluoch.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T05:27:38.125289Z",
     "iopub.status.busy": "2024-04-19T05:27:38.124869Z",
     "iopub.status.idle": "2024-04-19T05:27:38.135297Z",
     "shell.execute_reply": "2024-04-19T05:27:38.133715Z",
     "shell.execute_reply.started": "2024-04-19T05:27:38.125260Z"
    },
    "papermill": {
     "duration": 0.015292,
     "end_time": "2024-03-30T21:42:53.114503",
     "exception": false,
     "start_time": "2024-03-30T21:42:53.099211",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize Client Object\n",
    "client = bigquery.Client(project='project-adrian-julius-aluoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006314,
     "end_time": "2024-03-30T21:42:53.127485",
     "exception": false,
     "start_time": "2024-03-30T21:42:53.121171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<b><font size='6'>Build Webscraper : Appliances</font></b><br>\n",
    "<font size='4'>Scrape Jumia Products and Load data into Google BigQuery.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T05:27:38.137117Z",
     "iopub.status.busy": "2024-04-19T05:27:38.136797Z",
     "iopub.status.idle": "2024-04-19T05:30:51.658250Z",
     "shell.execute_reply": "2024-04-19T05:30:51.657093Z",
     "shell.execute_reply.started": "2024-04-19T05:27:38.137092Z"
    },
    "papermill": {
     "duration": 751.085533,
     "end_time": "2024-03-30T21:55:24.219489",
     "exception": false,
     "start_time": "2024-03-30T21:42:53.133956",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E-Commerce ====> https://www.jumia.co.ke/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:06<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.jumia.co.ke/ ----> RUNNING\n",
      "https://www.jumia.co.ke/ ----> DONE\n",
      "E-Commerce ====> https://www.jumia.ug/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:08<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.jumia.ug/ ----> RUNNING\n",
      "https://www.jumia.ug/ ----> DONE\n",
      "E-Commerce ====> https://www.jumia.com.ng/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:06<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.jumia.com.ng/ ----> RUNNING\n",
      "https://www.jumia.com.ng/ ----> DONE\n"
     ]
    }
   ],
   "source": [
    "stores = [\n",
    "    'https://www.jumia.co.ke/',\n",
    "    'https://www.jumia.com.ng/',\n",
    "]\n",
    "\n",
    "for store in stores:\n",
    "    print(f'E-Commerce ====> {store}')\n",
    "    bigdata = pd.DataFrame()\n",
    "    for i in trange(1,51):\n",
    "        url = str(store) + 'home-office-appliances/?page=' + str(i) + '#catalog-listing'\n",
    "            \n",
    "        content = requests.get(url).text\n",
    "        soup = BeautifulSoup(content,'lxml')\n",
    "            \n",
    "        pages = soup.find_all('div',class_='info')\n",
    "        data = pd.DataFrame()\n",
    "        for page in pages:\n",
    "            try:\n",
    "                item_name = page.find('h3',class_='name').text    \n",
    "                ratings = page.find('div',class_='stars _s').text   \n",
    "                try:\n",
    "                    initial_price = page.find('div',class_='old').text\n",
    "                except Exception as e:\n",
    "                    initial_price = np.NAN\n",
    "                final_price = page.find('div',class_='prc').text\n",
    "                last_scraped = datetime.now()\n",
    "                item_category = 'Appliances'\n",
    "                \n",
    "                # Create DataFrame to Temporarily store each listing\n",
    "                dataframe = pd.DataFrame({\n",
    "                                        'item_name':[item_name],\n",
    "                                        'item_category':[item_category],\n",
    "                                        'ratings':[ratings],\n",
    "                                        'initial_price':[initial_price],\n",
    "                                        'final_price':[final_price],\n",
    "                                        'last_scraped':[last_scraped]\n",
    "                                        })\n",
    "                data = pd.concat([data,dataframe],ignore_index=True)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pass\n",
    "                \n",
    "        bigdata = pd.concat([bigdata,data],ignore_index=True)\n",
    "\n",
    "    table_id = 'project-adrian-julius-aluoch.cronjobs.jumia_data'\n",
    "    job = client.load_table_from_dataframe(bigdata,table_id)\n",
    "\n",
    "    while job.state != 'DONE':\n",
    "        time.sleep(1)\n",
    "        job.reload()\n",
    "        print(f'{store} ----> {job.state}')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024955,
     "end_time": "2024-03-30T21:55:24.270007",
     "exception": false,
     "start_time": "2024-03-30T21:55:24.245052",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<b><font size='6'>Build Webscraper : Baby Products</font></b><br>\n",
    "<font size='4'>Scrape Jumia Products and Load data into Google BigQuery.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T05:30:51.661026Z",
     "iopub.status.busy": "2024-04-19T05:30:51.660592Z"
    },
    "papermill": {
     "duration": 766.41948,
     "end_time": "2024-03-30T22:08:10.714805",
     "exception": false,
     "start_time": "2024-03-30T21:55:24.295325",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E-Commerce ====> https://www.jumia.co.ke/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:21<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.jumia.co.ke/ ----> RUNNING\n",
      "https://www.jumia.co.ke/ ----> DONE\n",
      "E-Commerce ====> https://www.jumia.ug/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [00:13<01:04,  1.58s/it]"
     ]
    }
   ],
   "source": [
    "stores = [\n",
    "    'https://www.jumia.co.ke/',\n",
    "    'https://www.jumia.com.ng/'\n",
    "]\n",
    "\n",
    "for store in stores:\n",
    "    print(f'E-Commerce ====> {store}')\n",
    "    bigdata = pd.DataFrame()\n",
    "    for i in trange(1,51):\n",
    "        url = str(store) + 'baby-products/?page=' + str(i) + '#catalog-listing'\n",
    "            \n",
    "        content = requests.get(url).text\n",
    "        soup = BeautifulSoup(content,'lxml')\n",
    "            \n",
    "        pages = soup.find_all('div',class_='info')\n",
    "        data = pd.DataFrame()\n",
    "        for page in pages:\n",
    "            try:\n",
    "                item_name = page.find('h3',class_='name').text    \n",
    "                ratings = page.find('div',class_='stars _s').text   \n",
    "                try:\n",
    "                    initial_price = page.find('div',class_='old').text\n",
    "                except Exception as e:\n",
    "                    initial_price = np.NAN\n",
    "                final_price = page.find('div',class_='prc').text\n",
    "                last_scraped = datetime.now()\n",
    "                item_category = 'Baby Products'\n",
    "                \n",
    "                # Create DataFrame to Temporarily store each listing\n",
    "                dataframe = pd.DataFrame({\n",
    "                                        'item_name':[item_name],\n",
    "                                        'item_category':[item_category],\n",
    "                                        'ratings':[ratings],\n",
    "                                        'initial_price':[initial_price],\n",
    "                                        'final_price':[final_price],\n",
    "                                        'last_scraped':[last_scraped]\n",
    "                                        })\n",
    "                data = pd.concat([data,dataframe],ignore_index=True)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pass\n",
    "                \n",
    "        bigdata = pd.concat([bigdata,data],ignore_index=True)\n",
    "\n",
    "    table_id = 'project-adrian-julius-aluoch.cronjobs.jumia_data'\n",
    "    job = client.load_table_from_dataframe(bigdata,table_id)\n",
    "\n",
    "    while job.state != 'DONE':\n",
    "        time.sleep(1)\n",
    "        job.reload()\n",
    "        print(f'{store} ----> {job.state}')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034384,
     "end_time": "2024-03-30T22:08:10.784142",
     "exception": false,
     "start_time": "2024-03-30T22:08:10.749758",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<b><font size='6'>Build Webscraper : Computing</font></b><br>\n",
    "<font size='4'>Scrape Jumia Products and Load data into Google BigQuery.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 853.635901,
     "end_time": "2024-03-30T22:22:24.454316",
     "exception": false,
     "start_time": "2024-03-30T22:08:10.818415",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "stores = [\n",
    "    'https://www.jumia.co.ke/',\n",
    "    'https://www.jumia.com.ng/'\n",
    "]\n",
    "\n",
    "for store in stores:\n",
    "    print(f'E-Commerce ====> {store}')\n",
    "    bigdata = pd.DataFrame()\n",
    "    for i in trange(1,51):\n",
    "        url = str(store) + 'computing/?page=' + str(i) + '#catalog-listing'\n",
    "            \n",
    "        content = requests.get(url).text\n",
    "        soup = BeautifulSoup(content,'lxml')\n",
    "            \n",
    "        pages = soup.find_all('div',class_='info')\n",
    "        data = pd.DataFrame()\n",
    "        for page in pages:\n",
    "            try:\n",
    "                item_name = page.find('h3',class_='name').text    \n",
    "                ratings = page.find('div',class_='stars _s').text   \n",
    "                try:\n",
    "                    initial_price = page.find('div',class_='old').text\n",
    "                except Exception as e:\n",
    "                    initial_price = np.NAN\n",
    "                final_price = page.find('div',class_='prc').text\n",
    "                last_scraped = datetime.now()\n",
    "                item_category = 'Computing'\n",
    "                \n",
    "                # Create DataFrame to Temporarily store each listing\n",
    "                dataframe = pd.DataFrame({\n",
    "                                        'item_name':[item_name],\n",
    "                                        'item_category':[item_category],\n",
    "                                        'ratings':[ratings],\n",
    "                                        'initial_price':[initial_price],\n",
    "                                        'final_price':[final_price],\n",
    "                                        'last_scraped':[last_scraped]\n",
    "                                        })\n",
    "                data = pd.concat([data,dataframe],ignore_index=True)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pass\n",
    "                \n",
    "        bigdata = pd.concat([bigdata,data],ignore_index=True)\n",
    "\n",
    "    table_id = 'project-adrian-julius-aluoch.cronjobs.jumia_data'\n",
    "    job = client.load_table_from_dataframe(bigdata,table_id)\n",
    "\n",
    "    while job.state != 'DONE':\n",
    "        time.sleep(1)\n",
    "        job.reload()\n",
    "        print(f'{store} ----> {job.state}')       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.048175,
     "end_time": "2024-03-30T22:22:24.560467",
     "exception": false,
     "start_time": "2024-03-30T22:22:24.512292",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<b><font size='6'>Build Webscraper : Electronics</font></b><br>\n",
    "<font size='4'>Scrape Jumia Products and Load data into Google BigQuery.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 933.258395,
     "end_time": "2024-03-30T22:37:57.863764",
     "exception": false,
     "start_time": "2024-03-30T22:22:24.605369",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "stores = [\n",
    "    'https://www.jumia.co.ke/',\n",
    "    'https://www.jumia.ug/',\n",
    "    'https://www.jumia.com.ng/'\n",
    "]\n",
    "\n",
    "for store in stores:\n",
    "    print(f'E-Commerce ====> {store}')\n",
    "    bigdata = pd.DataFrame()\n",
    "    for i in trange(1,51):\n",
    "        url = str(store) + 'electronics/?page=' + str(i) + '#catalog-listing'\n",
    "            \n",
    "        content = requests.get(url).text\n",
    "        soup = BeautifulSoup(content,'lxml')\n",
    "            \n",
    "        pages = soup.find_all('div',class_='info')\n",
    "        data = pd.DataFrame()\n",
    "        for page in pages:\n",
    "            try:\n",
    "                item_name = page.find('h3',class_='name').text    \n",
    "                ratings = page.find('div',class_='stars _s').text   \n",
    "                try:\n",
    "                    initial_price = page.find('div',class_='old').text\n",
    "                except Exception as e:\n",
    "                    initial_price = np.NAN\n",
    "                final_price = page.find('div',class_='prc').text\n",
    "                last_scraped = datetime.now()\n",
    "                item_category = 'Electronics'\n",
    "                \n",
    "                # Create DataFrame to Temporarily store each listing\n",
    "                dataframe = pd.DataFrame({\n",
    "                                        'item_name':[item_name],\n",
    "                                        'item_category':[item_category],\n",
    "                                        'ratings':[ratings],\n",
    "                                        'initial_price':[initial_price],\n",
    "                                        'final_price':[final_price],\n",
    "                                        'last_scraped':[last_scraped]\n",
    "                                        })\n",
    "                data = pd.concat([data,dataframe],ignore_index=True)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pass\n",
    "                \n",
    "        bigdata = pd.concat([bigdata,data],ignore_index=True)\n",
    "\n",
    "    table_id = 'project-adrian-julius-aluoch.cronjobs.jumia_data'\n",
    "    job = client.load_table_from_dataframe(bigdata,table_id)\n",
    "\n",
    "    while job.state != 'DONE':\n",
    "        time.sleep(1)\n",
    "        job.reload()\n",
    "        print(f'{store} ----> {job.state}')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.053846,
     "end_time": "2024-03-30T22:37:57.970873",
     "exception": false,
     "start_time": "2024-03-30T22:37:57.917027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<b><font size='6'>Build Webscraper : Fashion</font></b><br>\n",
    "<font size='4'>Scrape Jumia Products and Load data into Google BigQuery.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 876.540057,
     "end_time": "2024-03-30T22:52:34.565722",
     "exception": false,
     "start_time": "2024-03-30T22:37:58.025665",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "stores = [\n",
    "    'https://www.jumia.co.ke/',\n",
    "    'https://www.jumia.com.ng/'\n",
    "]\n",
    "\n",
    "for store in stores:\n",
    "    print(f'E-Commerce ====> {store}')\n",
    "    bigdata = pd.DataFrame()\n",
    "    for i in trange(1,51):\n",
    "        url = str(store) + 'category-fashion-by-jumia/?page=' + str(i) + '#catalog-listing'\n",
    "            \n",
    "        content = requests.get(url).text\n",
    "        soup = BeautifulSoup(content,'lxml')\n",
    "            \n",
    "        pages = soup.find_all('div',class_='info')\n",
    "        data = pd.DataFrame()\n",
    "        for page in pages:\n",
    "            try:\n",
    "                item_name = page.find('h3',class_='name').text    \n",
    "                ratings = page.find('div',class_='stars _s').text   \n",
    "                try:\n",
    "                    initial_price = page.find('div',class_='old').text\n",
    "                except Exception as e:\n",
    "                    initial_price = np.NAN\n",
    "                final_price = page.find('div',class_='prc').text\n",
    "                last_scraped = datetime.now()\n",
    "                item_category = 'Fashion'\n",
    "                \n",
    "                # Create DataFrame to Temporarily store each listing\n",
    "                dataframe = pd.DataFrame({\n",
    "                                        'item_name':[item_name],\n",
    "                                        'item_category':[item_category],\n",
    "                                        'ratings':[ratings],\n",
    "                                        'initial_price':[initial_price],\n",
    "                                        'final_price':[final_price],\n",
    "                                        'last_scraped':[last_scraped]\n",
    "                                        })\n",
    "                data = pd.concat([data,dataframe],ignore_index=True)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pass\n",
    "                \n",
    "        bigdata = pd.concat([bigdata,data],ignore_index=True)\n",
    "\n",
    "    table_id = 'project-adrian-julius-aluoch.cronjobs.jumia_data'\n",
    "    job = client.load_table_from_dataframe(bigdata,table_id)\n",
    "\n",
    "    while job.state != 'DONE':\n",
    "        time.sleep(1)\n",
    "        job.reload()\n",
    "        print(f'{store} ----> {job.state}')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.063543,
     "end_time": "2024-03-30T22:52:34.693879",
     "exception": false,
     "start_time": "2024-03-30T22:52:34.630336",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<b><font size='6'>Build Webscraper : Grocery</font></b><br>\n",
    "<font size='4'>Scrape Jumia Products and Load data into Google BigQuery.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 749.815867,
     "end_time": "2024-03-30T23:05:04.574018",
     "exception": false,
     "start_time": "2024-03-30T22:52:34.758151",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "stores = [\n",
    "    'https://www.jumia.co.ke/',\n",
    "    'https://www.jumia.com.ng/'\n",
    "]\n",
    "\n",
    "for store in stores:\n",
    "    print(f'E-Commerce ====> {store}')\n",
    "    bigdata = pd.DataFrame()\n",
    "    for i in trange(1,51):\n",
    "        url = str(store) + 'groceries/?page=' + str(i) + '#catalog-listing'\n",
    "            \n",
    "        content = requests.get(url).text\n",
    "        soup = BeautifulSoup(content,'lxml')\n",
    "            \n",
    "        pages = soup.find_all('div',class_='info')\n",
    "        data = pd.DataFrame()\n",
    "        for page in pages:\n",
    "            try:\n",
    "                item_name = page.find('h3',class_='name').text    \n",
    "                ratings = page.find('div',class_='stars _s').text   \n",
    "                try:\n",
    "                    initial_price = page.find('div',class_='old').text\n",
    "                except Exception as e:\n",
    "                    initial_price = np.NAN\n",
    "                final_price = page.find('div',class_='prc').text\n",
    "                last_scraped = datetime.now()\n",
    "                item_category = 'Grocery'\n",
    "                \n",
    "                # Create DataFrame to Temporarily store each listing\n",
    "                dataframe = pd.DataFrame({\n",
    "                                        'item_name':[item_name],\n",
    "                                        'item_category':[item_category],\n",
    "                                        'ratings':[ratings],\n",
    "                                        'initial_price':[initial_price],\n",
    "                                        'final_price':[final_price],\n",
    "                                        'last_scraped':[last_scraped]\n",
    "                                        })\n",
    "                data = pd.concat([data,dataframe],ignore_index=True)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pass\n",
    "                \n",
    "        bigdata = pd.concat([bigdata,data],ignore_index=True)\n",
    "\n",
    "    table_id = 'project-adrian-julius-aluoch.cronjobs.jumia_data'\n",
    "    job = client.load_table_from_dataframe(bigdata,table_id)\n",
    "\n",
    "    while job.state != 'DONE':\n",
    "        time.sleep(1)\n",
    "        job.reload()\n",
    "        print(f'{store} ----> {job.state}')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.070991,
     "end_time": "2024-03-30T23:05:04.718288",
     "exception": false,
     "start_time": "2024-03-30T23:05:04.647297",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<b><font size='6'>Build Webscraper : Health and Beauty</font></b><br>\n",
    "<font size='4'>Scrape Jumia Products and Load data into Google BigQuery.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 810.309811,
     "end_time": "2024-03-30T23:18:35.099980",
     "exception": false,
     "start_time": "2024-03-30T23:05:04.790169",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "stores = [\n",
    "    'https://www.jumia.co.ke/',\n",
    "    'https://www.jumia.com.ng/'\n",
    "]\n",
    "\n",
    "for store in stores:\n",
    "    print(f'E-Commerce ====> {store}')\n",
    "    bigdata = pd.DataFrame()\n",
    "    for i in trange(1,51):\n",
    "        url = str(store) + 'health-beauty/?page=' + str(i) + '#catalog-listing'\n",
    "            \n",
    "        content = requests.get(url).text\n",
    "        soup = BeautifulSoup(content,'lxml')\n",
    "            \n",
    "        pages = soup.find_all('div',class_='info')\n",
    "        data = pd.DataFrame()\n",
    "        for page in pages:\n",
    "            try:\n",
    "                item_name = page.find('h3',class_='name').text    \n",
    "                ratings = page.find('div',class_='stars _s').text   \n",
    "                try:\n",
    "                    initial_price = page.find('div',class_='old').text\n",
    "                except Exception as e:\n",
    "                    initial_price = np.NAN\n",
    "                final_price = page.find('div',class_='prc').text\n",
    "                last_scraped = datetime.now()\n",
    "                item_category = 'Health & Beauty'\n",
    "                \n",
    "                # Create DataFrame to Temporarily store each listing\n",
    "                dataframe = pd.DataFrame({\n",
    "                                        'item_name':[item_name],\n",
    "                                        'item_category':[item_category],\n",
    "                                        'ratings':[ratings],\n",
    "                                        'initial_price':[initial_price],\n",
    "                                        'final_price':[final_price],\n",
    "                                        'last_scraped':[last_scraped]\n",
    "                                        })\n",
    "                data = pd.concat([data,dataframe],ignore_index=True)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pass\n",
    "                \n",
    "        bigdata = pd.concat([bigdata,data],ignore_index=True)\n",
    "\n",
    "    table_id = 'project-adrian-julius-aluoch.cronjobs.jumia_data'\n",
    "    job = client.load_table_from_dataframe(bigdata,table_id)\n",
    "\n",
    "    while job.state != 'DONE':\n",
    "        time.sleep(1)\n",
    "        job.reload()\n",
    "        print(f'{store} ----> {job.state}')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.08324,
     "end_time": "2024-03-30T23:18:35.265676",
     "exception": false,
     "start_time": "2024-03-30T23:18:35.182436",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<b><font size='6'>Build Webscraper : Home and Office</font></b><br>\n",
    "<font size='4'>Scrape Jumia Products and Load data into Google BigQuery.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 893.723763,
     "end_time": "2024-03-30T23:33:29.070641",
     "exception": false,
     "start_time": "2024-03-30T23:18:35.346878",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "stores = [\n",
    "    'https://www.jumia.co.ke/',\n",
    "    'https://www.jumia.com.ng/'\n",
    "]\n",
    "\n",
    "for store in stores:\n",
    "    print(f'E-Commerce ====> {store}')\n",
    "    bigdata = pd.DataFrame()\n",
    "    for i in trange(1,51):\n",
    "        url = str(store) + 'home-office/?page=' + str(i) + '#catalog-listing'\n",
    "            \n",
    "        content = requests.get(url).text\n",
    "        soup = BeautifulSoup(content,'lxml')\n",
    "            \n",
    "        pages = soup.find_all('div',class_='info')\n",
    "        data = pd.DataFrame()\n",
    "        for page in pages:\n",
    "            try:\n",
    "                item_name = page.find('h3',class_='name').text    \n",
    "                ratings = page.find('div',class_='stars _s').text   \n",
    "                try:\n",
    "                    initial_price = page.find('div',class_='old').text\n",
    "                except Exception as e:\n",
    "                    initial_price = np.NAN\n",
    "                final_price = page.find('div',class_='prc').text\n",
    "                last_scraped = datetime.now()\n",
    "                item_category = 'Home & Office'\n",
    "                \n",
    "                # Create DataFrame to Temporarily store each listing\n",
    "                dataframe = pd.DataFrame({\n",
    "                                        'item_name':[item_name],\n",
    "                                        'item_category':[item_category],\n",
    "                                        'ratings':[ratings],\n",
    "                                        'initial_price':[initial_price],\n",
    "                                        'final_price':[final_price],\n",
    "                                        'last_scraped':[last_scraped]\n",
    "                                        })\n",
    "                data = pd.concat([data,dataframe],ignore_index=True)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pass\n",
    "                \n",
    "        bigdata = pd.concat([bigdata,data],ignore_index=True)\n",
    "\n",
    "    table_id = 'project-adrian-julius-aluoch.cronjobs.jumia_data'\n",
    "    job = client.load_table_from_dataframe(bigdata,table_id)\n",
    "\n",
    "    while job.state != 'DONE':\n",
    "        time.sleep(1)\n",
    "        job.reload()\n",
    "        print(f'{store} ----> {job.state}')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.090923,
     "end_time": "2024-03-30T23:33:29.252681",
     "exception": false,
     "start_time": "2024-03-30T23:33:29.161758",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<b><font size='6'>Build Webscraper : Phones and Tablets</font></b><br>\n",
    "<font size='4'>Scrape Jumia Products and Load data into Google BigQuery.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 948.991145,
     "end_time": "2024-03-30T23:49:18.335180",
     "exception": false,
     "start_time": "2024-03-30T23:33:29.344035",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "stores = [\n",
    "    'https://www.jumia.co.ke/',\n",
    "    'https://www.jumia.com.ng/'\n",
    "]\n",
    "\n",
    "for store in stores:\n",
    "    print(f'E-Commerce ====> {store}')\n",
    "    bigdata = pd.DataFrame()\n",
    "    for i in trange(1,51):\n",
    "        url = str(store) + 'phones-tablets/?page=' + str(i) + '#catalog-listing'\n",
    "            \n",
    "        content = requests.get(url).text\n",
    "        soup = BeautifulSoup(content,'lxml')\n",
    "            \n",
    "        pages = soup.find_all('div',class_='info')\n",
    "        data = pd.DataFrame()\n",
    "        for page in pages:\n",
    "            try:\n",
    "                item_name = page.find('h3',class_='name').text    \n",
    "                ratings = page.find('div',class_='stars _s').text   \n",
    "                try:\n",
    "                    initial_price = page.find('div',class_='old').text\n",
    "                except Exception as e:\n",
    "                    initial_price = np.NAN\n",
    "                final_price = page.find('div',class_='prc').text\n",
    "                last_scraped = datetime.now()\n",
    "                item_category = 'Phones & Tablets'\n",
    "                \n",
    "                # Create DataFrame to Temporarily store each listing\n",
    "                dataframe = pd.DataFrame({\n",
    "                                        'item_name':[item_name],\n",
    "                                        'item_category':[item_category],\n",
    "                                        'ratings':[ratings],\n",
    "                                        'initial_price':[initial_price],\n",
    "                                        'final_price':[final_price],\n",
    "                                        'last_scraped':[last_scraped]\n",
    "                                        })\n",
    "                data = pd.concat([data,dataframe],ignore_index=True)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pass\n",
    "                \n",
    "        bigdata = pd.concat([bigdata,data],ignore_index=True)\n",
    "\n",
    "    table_id = 'project-adrian-julius-aluoch.cronjobs.jumia_data'\n",
    "    job = client.load_table_from_dataframe(bigdata,table_id)\n",
    "\n",
    "    while job.state != 'DONE':\n",
    "        time.sleep(1)\n",
    "        job.reload()\n",
    "        print(f'{store} ----> {job.state}')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.100559,
     "end_time": "2024-03-30T23:49:18.537431",
     "exception": false,
     "start_time": "2024-03-30T23:49:18.436872",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<b><font size='6'>Build Webscraper : Sporting</font></b><br>\n",
    "<font size='4'>Scrape Jumia Products and Load data into Google BigQuery.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 926.187632,
     "end_time": "2024-03-31T00:04:44.825560",
     "exception": false,
     "start_time": "2024-03-30T23:49:18.637928",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "stores = [\n",
    "    'https://www.jumia.co.ke/',\n",
    "    'https://www.jumia.com.ng/'\n",
    "]\n",
    "\n",
    "for store in stores:\n",
    "    print(f'E-Commerce ====> {store}')\n",
    "    bigdata = pd.DataFrame()\n",
    "    for i in trange(1,51):\n",
    "        url = str(store) + 'sporting-goods/?page=' + str(i) + '#catalog-listing'\n",
    "            \n",
    "        content = requests.get(url).text\n",
    "        soup = BeautifulSoup(content,'lxml')\n",
    "            \n",
    "        pages = soup.find_all('div',class_='info')\n",
    "        data = pd.DataFrame()\n",
    "        for page in pages:\n",
    "            try:\n",
    "                item_name = page.find('h3',class_='name').text    \n",
    "                ratings = page.find('div',class_='stars _s').text   \n",
    "                try:\n",
    "                    initial_price = page.find('div',class_='old').text\n",
    "                except Exception as e:\n",
    "                    initial_price = np.NAN\n",
    "                final_price = page.find('div',class_='prc').text\n",
    "                last_scraped = datetime.now()\n",
    "                item_category = 'Sporting'\n",
    "                \n",
    "                # Create DataFrame to Temporarily store each listing\n",
    "                dataframe = pd.DataFrame({\n",
    "                                        'item_name':[item_name],\n",
    "                                        'item_category':[item_category],\n",
    "                                        'ratings':[ratings],\n",
    "                                        'initial_price':[initial_price],\n",
    "                                        'final_price':[final_price],\n",
    "                                        'last_scraped':[last_scraped]\n",
    "                                        })\n",
    "                data = pd.concat([data,dataframe],ignore_index=True)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pass\n",
    "                \n",
    "        bigdata = pd.concat([bigdata,data],ignore_index=True)\n",
    "\n",
    "    table_id = 'project-adrian-julius-aluoch.cronjobs.jumia_data'\n",
    "    job = client.load_table_from_dataframe(bigdata,table_id)\n",
    "\n",
    "    while job.state != 'DONE':\n",
    "        time.sleep(1)\n",
    "        job.reload()\n",
    "        print(f'{store} ----> {job.state}')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.109185,
     "end_time": "2024-03-31T00:04:45.048362",
     "exception": false,
     "start_time": "2024-03-31T00:04:44.939177",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<b><font size='6'>Query data from Google Cloud Database.</font></b><br>\n",
    "<font size='4'>Run SQL query to get data from database.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.117379,
     "end_time": "2024-03-31T00:04:45.276345",
     "exception": false,
     "start_time": "2024-03-31T00:04:45.158966",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install db-dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_id = 'project-adrian-julius-aluoch.cronjobs.jumia_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.119234,
     "end_time": "2024-03-31T00:04:45.506374",
     "exception": false,
     "start_time": "2024-03-31T00:04:45.387140",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# query data from google cloud bigquery\n",
    "sql = (\n",
    "'SELECT *'\n",
    "'FROM `project-adrian-julius-aluoch.cronjobs.jumia_data`'\n",
    ")\n",
    "\n",
    "data = client.query(sql).to_dataframe()\n",
    "\n",
    "# Check Total Number of Duplicate Records\n",
    "duplicated = data.duplicated(subset=['item_name','item_category','ratings',\n",
    "                                     'initial_price','final_price'\n",
    "                                    ]).sum()\n",
    "print(f\"Initial Shape of Dataset : {data.shape}\\nTotal Duplicate Records : {duplicated:,.0f}\")\n",
    "\n",
    "# Remove Duplicate Records\n",
    "data.drop_duplicates(subset=['item_name','item_category','ratings',\n",
    "                             'initial_price','final_price'\n",
    "                            ],inplace=True)\n",
    "\n",
    "print(f\"Final Shape of Dataset : {data.shape}\")\n",
    "\n",
    "# Drop Original Real Estate Table \n",
    "table_id = 'project-adrian-julius-aluoch.cronjobs.jumia_data'\n",
    "client.delete_table(table_id)\n",
    "\n",
    "# Upload Final Real Estate Table\n",
    "job = client.load_table_from_dataframe(data,table_id)\n",
    "while job.state != 'DONE':\n",
    "    time.sleep(1)\n",
    "    job.reload()\n",
    "    print(job.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.133085,
     "end_time": "2024-03-31T00:05:01.186066",
     "exception": false,
     "start_time": "2024-03-31T00:05:01.052981",
     "status": "completed"
    },
    "scrolled": true,
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "display = data.iloc[:50].sort_values(by='last_scraped').reset_index(drop=True)\n",
    "display"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4489532,
     "sourceId": 7692588,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DataNerd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8537.171963,
   "end_time": "2024-03-31T00:05:03.013573",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-30T21:42:45.841610",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
